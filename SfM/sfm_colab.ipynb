{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {"id": "title"},
   "source": [
    "# Structure from Motion (SfM)\n",
    "## A Complete Implementation Guide for Google Colab\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/)\n",
    "\n",
    "**Structure from Motion** recovers the 3D structure of a scene and the camera poses that observed it, using only a collection of 2D images. It is the backbone of systems like Google Street View, Photogrammetry software, and autonomous vehicle mapping.\n",
    "\n",
    "---\n",
    "\n",
    "## The SfM Pipeline\n",
    "\n",
    "```\n",
    "  Images\n",
    "    │\n",
    "    ▼\n",
    "  Feature Detection & Description   (SIFT keypoints + descriptors)\n",
    "    │\n",
    "    ▼\n",
    "  Feature Matching                  (cross-image correspondences)\n",
    "    │\n",
    "    ▼\n",
    "  Geometric Verification            (RANSAC + Fundamental/Essential Matrix)\n",
    "    │\n",
    "    ▼\n",
    "  Camera Pose Recovery              (decompose Essential Matrix → R, t)\n",
    "    │\n",
    "    ▼\n",
    "  Triangulation                     (3D point cloud from 2D correspondences)\n",
    "    │\n",
    "    ▼\n",
    "  Bundle Adjustment                 (jointly refine cameras + 3D points)\n",
    "    │\n",
    "    ▼\n",
    "  3D Reconstruction                 (sparse point cloud + camera poses)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Notebook Contents\n",
    "| # | Topic |\n",
    "|---|---|\n",
    "| 1 | Setup & GPU check |\n",
    "| 2 | The pinhole camera model |\n",
    "| 3 | Feature detection with SIFT |\n",
    "| 4 | Feature matching & ratio test |\n",
    "| 5 | The Fundamental Matrix |\n",
    "| 6 | The Essential Matrix |\n",
    "| 7 | RANSAC for robust estimation |\n",
    "| 8 | Camera pose recovery (R, t) |\n",
    "| 9 | Triangulation |\n",
    "| 10 | Incremental SfM on real images |\n",
    "| 11 | Bundle Adjustment |\n",
    "| 12 | 3D Point Cloud Visualization |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"id": "s1"},
   "source": ["## 1. Setup & GPU Check"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "setup"},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install opencv-contrib-python-headless --quiet   # SIFT + extra matchers\n",
    "!pip install scipy matplotlib numpy tqdm --quiet\n",
    "!pip install open3d --quiet                           # 3D point cloud viewer\n",
    "!pip install plotly --quiet                           # interactive 3D plots\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "imports"},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import plotly.graph_objects as go\n",
    "from scipy.optimize import least_squares\n",
    "from scipy.sparse import lil_matrix\n",
    "from itertools import combinations\n",
    "import os, warnings, time\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check cv2 build (must have SIFT)\n",
    "print(f'OpenCV version : {cv2.__version__}')\n",
    "try:\n",
    "    sift_test = cv2.SIFT_create()\n",
    "    print('SIFT           : ✅ available')\n",
    "except:\n",
    "    print('SIFT           : ❌ not available — reinstall opencv-contrib-python-headless')\n",
    "\n",
    "import torch\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Device         : {DEVICE}')\n",
    "if DEVICE == 'cuda':\n",
    "    print(f'GPU            : {torch.cuda.get_device_name(0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"id": "s2"},
   "source": [
    "## 2. The Pinhole Camera Model\n",
    "\n",
    "Every SfM system starts with a mathematical model of how a 3D point projects onto a 2D image plane.\n",
    "\n",
    "### 2.1 Projection Formula\n",
    "\n",
    "A 3D world point $\\mathbf{X} = [X, Y, Z]^T$ projects to a 2D image pixel $\\mathbf{x} = [u, v]^T$ via:\n",
    "\n",
    "$$\\lambda \\begin{bmatrix} u \\\\ v \\\\ 1 \\end{bmatrix} = \\mathbf{K} \\begin{bmatrix} \\mathbf{R} & \\mathbf{t} \\end{bmatrix} \\begin{bmatrix} X \\\\ Y \\\\ Z \\\\ 1 \\end{bmatrix}$$\n",
    "\n",
    "where $\\lambda$ is the depth, $\\mathbf{K}$ is the **intrinsic matrix**, and $[\\mathbf{R} | \\mathbf{t}]$ is the **extrinsic matrix**.\n",
    "\n",
    "### 2.2 The Intrinsic Matrix K\n",
    "\n",
    "$$\\mathbf{K} = \\begin{bmatrix} f_x & s & c_x \\\\ 0 & f_y & c_y \\\\ 0 & 0 & 1 \\end{bmatrix}$$\n",
    "\n",
    "- $f_x, f_y$ — focal lengths in pixels (typically equal for square pixels)\n",
    "- $c_x, c_y$ — principal point (usually image center)\n",
    "- $s$ — skew (almost always 0)\n",
    "\n",
    "### 2.3 The Extrinsic Matrix [R | t]\n",
    "\n",
    "Transforms points from **world coordinates** into **camera coordinates**:\n",
    "\n",
    "$$\\mathbf{X}_{cam} = \\mathbf{R} \\mathbf{X}_{world} + \\mathbf{t}$$\n",
    "\n",
    "- $\\mathbf{R}$ — 3×3 rotation matrix (camera orientation)\n",
    "- $\\mathbf{t}$ — 3×1 translation vector (camera position in world, negated)\n",
    "\n",
    "**Goal of SfM:** Recover $\\mathbf{K}$, $\\mathbf{R}_i$, $\\mathbf{t}_i$ for each camera $i$, and the 3D positions $\\mathbf{X}_j$ of all scene points $j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "camera_model"},
   "outputs": [],
   "source": [
    "# ── Camera model implementation ───────────────────────────────────\n",
    "\n",
    "def build_K(fx, fy, cx, cy, skew=0.0):\n",
    "    \"\"\"Build the 3×3 intrinsic matrix K.\"\"\"\n",
    "    return np.array([[fx, skew, cx],\n",
    "                     [ 0,   fy, cy],\n",
    "                     [ 0,    0,  1]], dtype=np.float64)\n",
    "\n",
    "\n",
    "def project_points(X_world, K, R, t):\n",
    "    \"\"\"\n",
    "    Project 3D world points into 2D image pixels.\n",
    "\n",
    "    Args:\n",
    "        X_world : (N, 3) world coordinates\n",
    "        K       : (3, 3) intrinsic matrix\n",
    "        R       : (3, 3) rotation matrix\n",
    "        t       : (3,)   translation vector\n",
    "    Returns:\n",
    "        pts2d   : (N, 2) pixel coordinates [u, v]\n",
    "        depths  : (N,)   depth values (Z in camera frame)\n",
    "    \"\"\"\n",
    "    # Transform to camera coordinates\n",
    "    X_cam = (R @ X_world.T + t.reshape(3,1)).T   # (N, 3)\n",
    "    depths = X_cam[:, 2]\n",
    "\n",
    "    # Perspective divide → normalised image coords\n",
    "    x_norm = X_cam[:, 0] / (X_cam[:, 2] + 1e-10)\n",
    "    y_norm = X_cam[:, 1] / (X_cam[:, 2] + 1e-10)\n",
    "\n",
    "    # Apply K\n",
    "    u = K[0,0] * x_norm + K[0,2]\n",
    "    v = K[1,1] * y_norm + K[1,2]\n",
    "\n",
    "    return np.stack([u, v], axis=-1), depths\n",
    "\n",
    "\n",
    "def unproject_pixel(px, K):\n",
    "    \"\"\"\n",
    "    Convert pixel (u,v) to normalised camera ray (x_n, y_n, 1).\n",
    "    This is the inverse of K: x_norm = K^{-1} * [u, v, 1]^T\n",
    "    \"\"\"\n",
    "    K_inv = np.linalg.inv(K)\n",
    "    px_h  = np.array([px[0], px[1], 1.0])\n",
    "    return K_inv @ px_h\n",
    "\n",
    "\n",
    "# ── Visualise projection ─────────────────────────────────────────\n",
    "\n",
    "# Example: synthetic 3D scene\n",
    "np.random.seed(0)\n",
    "N_pts   = 50\n",
    "X_world = np.random.randn(N_pts, 3) * 2.0\n",
    "X_world[:, 2] += 8.0         # place scene ~8 m in front of camera\n",
    "\n",
    "# Camera looking straight down -Z (identity rotation, at origin)\n",
    "K_demo = build_K(fx=800, fy=800, cx=320, cy=240)\n",
    "R_demo = np.eye(3)\n",
    "t_demo = np.zeros(3)\n",
    "\n",
    "pts2d, depths = project_points(X_world, K_demo, R_demo, t_demo)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.scatter(X_world[:,0], X_world[:,2], c=depths, cmap='plasma', s=40, zorder=3)\n",
    "ax.set_xlabel('X (m)'); ax.set_ylabel('Z depth (m)')\n",
    "ax.set_title('3D Scene (side view)', fontweight='bold')\n",
    "ax.axvline(0, color='k', lw=1, ls='--'); ax.axhline(0, color='k', lw=1, ls='--')\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "ax = axes[1]\n",
    "valid = (depths > 0) & (pts2d[:,0] >= 0) & (pts2d[:,0] < 640) & \\\n",
    "        (pts2d[:,1] >= 0) & (pts2d[:,1] < 480)\n",
    "sc = ax.scatter(pts2d[valid,0], pts2d[valid,1], c=depths[valid], cmap='plasma', s=40)\n",
    "ax.set_xlim(0, 640); ax.set_ylim(480, 0)\n",
    "ax.set_xlabel('u (pixels)'); ax.set_ylabel('v (pixels)')\n",
    "ax.set_title('Projected Image (640×480)', fontweight='bold')\n",
    "ax.add_patch(plt.Rectangle((0,0), 640, 480, fill=False, ec='red', lw=2))\n",
    "plt.colorbar(sc, ax=ax, label='Depth (m)')\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('Pinhole Camera Projection', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'K =\\n{K_demo}')\n",
    "print(f'\\nProjected {valid.sum()}/{N_pts} points into the image frame')\n",
    "print(f'Depth range: [{depths.min():.2f}, {depths.max():.2f}] m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"id": "s3"},
   "source": [
    "## 3. Feature Detection with SIFT\n",
    "\n",
    "SfM requires **repeatable, distinctive keypoints** that can be matched across different viewpoints and lighting conditions.\n",
    "\n",
    "**SIFT (Scale-Invariant Feature Transform)** is the gold standard:\n",
    "- Finds keypoints at multiple scales using a **Difference of Gaussians (DoG)** pyramid\n",
    "- Each keypoint gets a **128-dimensional descriptor** encoding local gradient orientations\n",
    "- Invariant to: scale, rotation, illumination, and moderate viewpoint changes\n",
    "\n",
    "### DoG Scale-Space\n",
    "\n",
    "$$D(x, y, \\sigma) = L(x, y, k\\sigma) - L(x, y, \\sigma)$$\n",
    "\n",
    "where $L(x, y, \\sigma) = G(x, y, \\sigma) * I(x, y)$ is the image convolved with a Gaussian at scale $\\sigma$. Keypoints are local extrema (maxima/minima) in this 3D scale-space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "synthetic_images"},
   "outputs": [],
   "source": [
    "# ── Generate synthetic test scene ────────────────────────────────\n",
    "# We create a textured planar scene viewed from two angles.\n",
    "# Using synthetic data means we have perfect ground truth for validation.\n",
    "\n",
    "def make_textured_image(H=480, W=640, seed=0):\n",
    "    \"\"\"Synthetic scene: checkerboard + random circles + gradient.\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    img = np.zeros((H, W, 3), dtype=np.uint8)\n",
    "\n",
    "    # Checkerboard background\n",
    "    sq = 40\n",
    "    for i in range(0, H, sq):\n",
    "        for j in range(0, W, sq):\n",
    "            if (i//sq + j//sq) % 2 == 0:\n",
    "                img[i:i+sq, j:j+sq] = [200, 200, 200]\n",
    "            else:\n",
    "                img[i:i+sq, j:j+sq] = [60, 60, 60]\n",
    "\n",
    "    # Coloured circles\n",
    "    for _ in range(30):\n",
    "        x, y = rng.integers(50, W-50), rng.integers(50, H-50)\n",
    "        r    = rng.integers(15, 50)\n",
    "        c    = rng.integers(80, 255, 3)\n",
    "        cv2.circle(img, (x, y), r, c.tolist(), -1)\n",
    "\n",
    "    # Text features for distinctiveness\n",
    "    for i, ch in enumerate('SfM DEMO'):\n",
    "        cv2.putText(img, ch, (60 + i*65, 60), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    1.8, (255,255,0), 3)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def make_view_pair(img, angle_deg=15.0, scale=1.0, tx=40, ty=20):\n",
    "    \"\"\"\n",
    "    Simulate a second camera view via a homography (planar scene assumption).\n",
    "    Returns (img1, img2, H_gt) where H_gt is the ground-truth homography.\n",
    "    \"\"\"\n",
    "    H, W = img.shape[:2]\n",
    "    cx, cy = W/2, H/2\n",
    "\n",
    "    angle_rad = np.deg2rad(angle_deg)\n",
    "    cos_a, sin_a = np.cos(angle_rad), np.sin(angle_rad)\n",
    "\n",
    "    # Rotation + scale + translation homography\n",
    "    H_gt = np.array([\n",
    "        [scale*cos_a, -scale*sin_a, tx + cx*(1 - scale*cos_a) + cy*scale*sin_a],\n",
    "        [scale*sin_a,  scale*cos_a, ty + cy*(1 - scale*cos_a) - cx*scale*sin_a],\n",
    "        [0,            0,           1]\n",
    "    ], dtype=np.float64)\n",
    "\n",
    "    img2 = cv2.warpPerspective(img, H_gt, (W, H),\n",
    "                                flags=cv2.INTER_LINEAR,\n",
    "                                borderMode=cv2.BORDER_CONSTANT,\n",
    "                                borderValue=(128,128,128))\n",
    "    return img, img2, H_gt\n",
    "\n",
    "\n",
    "# Create images\n",
    "base_img = make_textured_image(seed=5)\n",
    "img1, img2, H_true = make_view_pair(base_img, angle_deg=12, scale=0.92, tx=50, ty=30)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "axes[0].imshow(img1); axes[0].set_title('Image 1 — Reference view', fontweight='bold'); axes[0].axis('off')\n",
    "axes[1].imshow(img2); axes[1].set_title('Image 2 — Rotated / translated view', fontweight='bold'); axes[1].axis('off')\n",
    "plt.suptitle('Synthetic Image Pair', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "sift"},
   "outputs": [],
   "source": [
    "# ── SIFT Feature Detection ────────────────────────────────────────\n",
    "\n",
    "def detect_features(img, n_features=2000):\n",
    "    \"\"\"\n",
    "    Detect SIFT keypoints and compute descriptors.\n",
    "\n",
    "    Returns:\n",
    "        kps   : list of cv2.KeyPoint\n",
    "        descs : (N, 128) float32 descriptor array\n",
    "        pts   : (N, 2) float32 pixel coordinates\n",
    "    \"\"\"\n",
    "    sift  = cv2.SIFT_create(nfeatures=n_features)\n",
    "    gray  = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) if img.ndim == 3 else img\n",
    "    kps, descs = sift.detectAndCompute(gray, None)\n",
    "\n",
    "    if descs is None:\n",
    "        return [], np.empty((0,128), np.float32), np.empty((0,2))\n",
    "\n",
    "    pts = np.array([kp.pt for kp in kps], dtype=np.float32)\n",
    "    return kps, descs, pts\n",
    "\n",
    "\n",
    "def visualise_keypoints(img, kps, title='Keypoints', max_show=500):\n",
    "    \"\"\"Draw keypoints with scale and orientation circles.\"\"\"\n",
    "    vis = cv2.drawKeypoints(img, kps[:max_show], None,\n",
    "                             flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "    return cv2.cvtColor(vis, cv2.COLOR_BGR2RGB) if vis.ndim == 3 else vis\n",
    "\n",
    "\n",
    "kps1, descs1, pts1 = detect_features(img1)\n",
    "kps2, descs2, pts2 = detect_features(img2)\n",
    "\n",
    "print(f'Image 1: {len(kps1)} keypoints detected')\n",
    "print(f'Image 2: {len(kps2)} keypoints detected')\n",
    "print(f'Descriptor shape: {descs1.shape}  (N × 128)')\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "axes[0].imshow(visualise_keypoints(img1, kps1))\n",
    "axes[0].set_title(f'Image 1 — {len(kps1)} SIFT keypoints\\n(circles = scale, line = orientation)', fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "axes[1].imshow(visualise_keypoints(img2, kps2))\n",
    "axes[1].set_title(f'Image 2 — {len(kps2)} SIFT keypoints', fontweight='bold')\n",
    "axes[1].axis('off')\n",
    "plt.suptitle('SIFT Feature Detection', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Distribution of keypoint scales\n",
    "scales1 = [kp.size for kp in kps1]\n",
    "fig, ax = plt.subplots(figsize=(8, 3))\n",
    "ax.hist(scales1, bins=40, color='steelblue', edgecolor='white', lw=0.5)\n",
    "ax.set_xlabel('Keypoint scale (pixels)')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('SIFT Scale Distribution — keypoints detected at multiple scales', fontweight='bold')\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"id": "s4"},
   "source": [
    "## 4. Feature Matching & Lowe's Ratio Test\n",
    "\n",
    "To match keypoints across images, we find the **nearest neighbour** in descriptor space using Euclidean distance. However, naive nearest-neighbour matching produces many false matches.\n",
    "\n",
    "### Lowe's Ratio Test\n",
    "\n",
    "For each descriptor in image 1, find the two nearest descriptors in image 2. Accept the match only if:\n",
    "\n",
    "$$\\frac{d_1}{d_2} < \\tau \\quad (\\text{typically } \\tau = 0.75)$$\n",
    "\n",
    "where $d_1$ is the distance to the best match and $d_2$ is the distance to the second-best match.\n",
    "\n",
    "**Intuition:** If the best match is significantly closer than the second-best, it is likely a true match. If both distances are similar, the descriptor is ambiguous and the match is unreliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "matching"},
   "outputs": [],
   "source": [
    "def match_features(descs1, descs2, ratio_thresh=0.75):\n",
    "    \"\"\"\n",
    "    Match SIFT descriptors using FLANN + Lowe's ratio test.\n",
    "\n",
    "    Returns:\n",
    "        good_matches : list of cv2.DMatch (filtered)\n",
    "        all_matches  : list of cv2.DMatch (before ratio test)\n",
    "    \"\"\"\n",
    "    # FLANN (Fast Library for Approximate Nearest Neighbors)\n",
    "    # Much faster than brute-force for large descriptor sets\n",
    "    FLANN_INDEX_KDTREE = 1\n",
    "    index_params  = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "    search_params = dict(checks=50)\n",
    "    flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "    # knnMatch: return the 2 nearest neighbours for ratio test\n",
    "    matches_knn = flann.knnMatch(descs1, descs2, k=2)\n",
    "\n",
    "    good_matches = []\n",
    "    for m, n in matches_knn:\n",
    "        if m.distance < ratio_thresh * n.distance:\n",
    "            good_matches.append(m)\n",
    "\n",
    "    return good_matches, matches_knn\n",
    "\n",
    "\n",
    "def draw_matches_vis(img1, kps1, img2, kps2, matches, max_show=80, title=''):\n",
    "    \"\"\"Draw match lines between two images.\"\"\"\n",
    "    vis = cv2.drawMatches(img1, kps1, img2, kps2,\n",
    "                           matches[:max_show], None,\n",
    "                           flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS,\n",
    "                           matchColor=(0, 255, 0),\n",
    "                           singlePointColor=(200, 200, 200))\n",
    "    return cv2.cvtColor(vis, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "good_matches, all_matches = match_features(descs1, descs2, ratio_thresh=0.75)\n",
    "\n",
    "print(f'Total matches (before ratio test): {len(all_matches)}')\n",
    "print(f'Good matches  (after  ratio test): {len(good_matches)}')\n",
    "print(f'Rejection rate: {100*(1 - len(good_matches)/len(all_matches)):.1f}%')\n",
    "\n",
    "# Visualise\n",
    "fig, axes = plt.subplots(1, 1, figsize=(16, 6))\n",
    "axes.imshow(draw_matches_vis(img1, kps1, img2, kps2, good_matches))\n",
    "axes.set_title(f'SIFT Matches after Lowe\\'s Ratio Test — {len(good_matches)} matches shown',\n",
    "               fontweight='bold')\n",
    "axes.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show ratio test effect\n",
    "ratios = [m[0].distance / m[1].distance for m in all_matches if len(m) == 2]\n",
    "fig, ax = plt.subplots(figsize=(8, 3.5))\n",
    "ax.hist(ratios, bins=50, color='steelblue', edgecolor='white', lw=0.5)\n",
    "ax.axvline(0.75, color='red', lw=2, ls='--', label='Ratio threshold = 0.75')\n",
    "ax.fill_betweenx([0, ax.get_ylim()[1] if ax.get_ylim()[1] > 0 else 200],\n",
    "                  0, 0.75, alpha=0.15, color='green', label='Accepted')\n",
    "ax.set_xlabel('Distance ratio d₁/d₂')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title(\"Lowe's Ratio Test Distribution\", fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"id": "s5"},
   "source": [
    "## 5. The Fundamental Matrix\n",
    "\n",
    "Even after the ratio test, matches still contain outliers. The **Fundamental Matrix** $\\mathbf{F}$ encodes the **epipolar constraint** — a geometric consistency check between any two views.\n",
    "\n",
    "### The Epipolar Constraint\n",
    "\n",
    "For any true correspondence $(\\mathbf{x}, \\mathbf{x}')$ between image 1 and image 2:\n",
    "\n",
    "$$\\mathbf{x}'^T \\mathbf{F} \\mathbf{x} = 0$$\n",
    "\n",
    "This means that for a point $\\mathbf{x}$ in image 1, its match in image 2 must lie on the **epipolar line** $\\mathbf{l}' = \\mathbf{F}\\mathbf{x}$.\n",
    "\n",
    "### Geometry: Epipoles and Epipolar Lines\n",
    "\n",
    "- **Epipole $\\mathbf{e}$**: The projection of camera 2's centre into image 1 (and vice versa)\n",
    "- **Epipolar plane**: The plane through the two camera centres and a 3D point\n",
    "- **Epipolar line**: The intersection of the epipolar plane with each image plane\n",
    "\n",
    "All epipolar lines in image 1 pass through the epipole $\\mathbf{e}_1$; all lines in image 2 pass through $\\mathbf{e}_2$.\n",
    "\n",
    "### Properties of F\n",
    "- 3×3 matrix, rank 2 (det(**F**) = 0)\n",
    "- 7 degrees of freedom (9 elements, scale-invariant, rank-2 constraint)\n",
    "- Can be estimated from **7 or 8 point correspondences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "fundamental"},
   "outputs": [],
   "source": [
    "def extract_match_points(kps1, kps2, matches):\n",
    "    \"\"\"Extract matched pixel coordinates from keypoints and match list.\"\"\"\n",
    "    pts1 = np.float32([kps1[m.queryIdx].pt for m in matches])\n",
    "    pts2 = np.float32([kps2[m.trainIdx].pt for m in matches])\n",
    "    return pts1, pts2\n",
    "\n",
    "\n",
    "def epipolar_distance(F, pts1, pts2):\n",
    "    \"\"\"\n",
    "    Compute symmetric epipolar distance for each correspondence.\n",
    "    A perfect match has distance = 0.\n",
    "\n",
    "    Symmetric distance = d(x', Fx)^2 + d(x, F^T x')^2\n",
    "    where d(p, l) is the point-to-line distance.\n",
    "    \"\"\"\n",
    "    pts1_h = np.column_stack([pts1, np.ones(len(pts1))])\n",
    "    pts2_h = np.column_stack([pts2, np.ones(len(pts2))])\n",
    "\n",
    "    # Epipolar lines in image 2: l2 = F @ x1\n",
    "    l2 = (F @ pts1_h.T).T          # (N, 3)\n",
    "    # Epipolar lines in image 1: l1 = F^T @ x2\n",
    "    l1 = (F.T @ pts2_h.T).T        # (N, 3)\n",
    "\n",
    "    # Point-to-line distances\n",
    "    def pt_line_dist(pts_h, lines):\n",
    "        num = np.abs(np.sum(pts_h * lines, axis=1))\n",
    "        den = np.sqrt(lines[:,0]**2 + lines[:,1]**2 + 1e-10)\n",
    "        return num / den\n",
    "\n",
    "    d1 = pt_line_dist(pts1_h, l1)\n",
    "    d2 = pt_line_dist(pts2_h, l2)\n",
    "    return d1 + d2\n",
    "\n",
    "\n",
    "# Extract matched points\n",
    "match_pts1, match_pts2 = extract_match_points(kps1, kps2, good_matches)\n",
    "\n",
    "# Estimate F with RANSAC (automatically discards outliers)\n",
    "F, inlier_mask = cv2.findFundamentalMat(\n",
    "    match_pts1, match_pts2,\n",
    "    method=cv2.FM_RANSAC,\n",
    "    ransacReprojThreshold=3.0,   # pixels\n",
    "    confidence=0.999\n",
    ")\n",
    "\n",
    "inliers = inlier_mask.ravel().astype(bool)\n",
    "pts1_in = match_pts1[inliers]\n",
    "pts2_in = match_pts2[inliers]\n",
    "\n",
    "print(f'Fundamental Matrix F:')\n",
    "print(np.round(F, 6))\n",
    "print(f'\\nRank of F: {np.linalg.matrix_rank(F)}  (should be 2)')\n",
    "print(f'det(F)   : {np.linalg.det(F):.2e}  (should be ~0)')\n",
    "print(f'\\nInliers : {inliers.sum()} / {len(good_matches)}')\n",
    "\n",
    "epi_dists = epipolar_distance(F, pts1_in, pts2_in)\n",
    "print(f'Mean symmetric epipolar distance (inliers): {epi_dists.mean():.3f} px')\n",
    "\n",
    "# ── Draw epipolar lines ──────────────────────────────────────────\n",
    "def draw_epipolar_lines(img1, img2, F, pts1, pts2, n_lines=12):\n",
    "    \"\"\"Draw corresponding epipolar lines on both images.\"\"\"\n",
    "    H, W = img1.shape[:2]\n",
    "    vis1 = img1.copy(); vis2 = img2.copy()\n",
    "\n",
    "    idx    = np.random.choice(len(pts1), min(n_lines, len(pts1)), replace=False)\n",
    "    colors = plt.cm.hsv(np.linspace(0, 1, len(idx)))[:, :3]\n",
    "    colors = (colors * 255).astype(int)\n",
    "\n",
    "    for i, (ii, col) in enumerate(zip(idx, colors)):\n",
    "        c = tuple(col.tolist())\n",
    "        p1, p2 = tuple(pts1[ii].astype(int)), tuple(pts2[ii].astype(int))\n",
    "\n",
    "        # Epipolar line in image 2\n",
    "        l2 = F @ np.array([pts1[ii,0], pts1[ii,1], 1.0])\n",
    "        x0, y0 = 0, int(-l2[2] / (l2[1] + 1e-10))\n",
    "        x1, y1 = W, int(-(l2[2] + l2[0]*W) / (l2[1] + 1e-10))\n",
    "        cv2.line(vis2, (x0,y0), (x1,y1), c, 1)\n",
    "        cv2.circle(vis2, p2, 6, c, -1)\n",
    "\n",
    "        # Epipolar line in image 1\n",
    "        l1 = F.T @ np.array([pts2[ii,0], pts2[ii,1], 1.0])\n",
    "        x0, y0 = 0, int(-l1[2] / (l1[1] + 1e-10))\n",
    "        x1, y1 = W, int(-(l1[2] + l1[0]*W) / (l1[1] + 1e-10))\n",
    "        cv2.line(vis1, (x0,y0), (x1,y1), c, 1)\n",
    "        cv2.circle(vis1, p1, 6, c, -1)\n",
    "\n",
    "    return cv2.cvtColor(vis1, cv2.COLOR_BGR2RGB), cv2.cvtColor(vis2, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "epi1, epi2 = draw_epipolar_lines(img1, img2, F, pts1_in, pts2_in, n_lines=15)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "axes[0].imshow(epi1); axes[0].set_title('Epipolar Lines — Image 1\\n(each colour = one correspondence)', fontweight='bold'); axes[0].axis('off')\n",
    "axes[1].imshow(epi2); axes[1].set_title('Epipolar Lines — Image 2\\n(point must lie on coloured line)', fontweight='bold'); axes[1].axis('off')\n",
    "plt.suptitle('Epipolar Geometry from Fundamental Matrix', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"id": "s6"},
   "source": [
    "## 6. The Essential Matrix\n",
    "\n",
    "The **Essential Matrix** $\\mathbf{E}$ is the calibrated version of $\\mathbf{F}$. While $\\mathbf{F}$ works in pixel coordinates, $\\mathbf{E}$ works in **normalised camera coordinates** (after removing the effect of $\\mathbf{K}$):\n",
    "\n",
    "$$\\mathbf{E} = \\mathbf{K}'^T \\mathbf{F} \\mathbf{K}$$\n",
    "\n",
    "The constraint becomes:\n",
    "\n",
    "$$\\hat{\\mathbf{x}}'^T \\mathbf{E} \\hat{\\mathbf{x}} = 0$$\n",
    "\n",
    "where $\\hat{\\mathbf{x}} = \\mathbf{K}^{-1}\\mathbf{x}$ are normalised coordinates.\n",
    "\n",
    "### Why E is more useful than F\n",
    "\n",
    "$\\mathbf{E}$ encodes **metric information** — it directly gives us the rotation $\\mathbf{R}$ and (unit) translation $\\mathbf{t}$ between cameras via SVD:\n",
    "\n",
    "$$\\mathbf{E} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T \\quad\\text{where}\\quad \\mathbf{\\Sigma} = \\text{diag}(\\sigma, \\sigma, 0)$$\n",
    "\n",
    "Properties:\n",
    "- Two singular values are equal, one is zero\n",
    "- 5 degrees of freedom (vs 7 for F)\n",
    "- Can be estimated from just **5 point correspondences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "essential"},
   "outputs": [],
   "source": [
    "# ── Camera intrinsics ────────────────────────────────────────────\n",
    "# For our synthetic images (640×480)\n",
    "H_img, W_img = img1.shape[:2]\n",
    "focal = max(H_img, W_img) * 1.2   # reasonable estimate: ~768 px\n",
    "K = build_K(fx=focal, fy=focal, cx=W_img/2, cy=H_img/2)\n",
    "\n",
    "print(f'Camera Intrinsics K:')\n",
    "print(K)\n",
    "\n",
    "# ── Compute Essential Matrix from F and K ─────────────────────────\n",
    "E_from_F = K.T @ F @ K\n",
    "\n",
    "# Or estimate E directly (5-point algorithm via RANSAC)\n",
    "E_direct, e_mask = cv2.findEssentialMat(\n",
    "    pts1_in, pts2_in, K,\n",
    "    method=cv2.RANSAC,\n",
    "    prob=0.999,\n",
    "    threshold=1.0\n",
    ")\n",
    "\n",
    "# ── Validate E using SVD ─────────────────────────────────────────\n",
    "def check_essential(E):\n",
    "    \"\"\"Verify E properties: rank 2, two equal singular values.\"\"\"\n",
    "    U, sv, Vt = np.linalg.svd(E)\n",
    "    sv_norm   = sv / sv[0]  # normalise\n",
    "    print(f'  Singular values (normalised): {sv_norm.round(4)}')\n",
    "    print(f'  Ratio σ1/σ2: {sv[0]/sv[1]:.4f}  (ideal = 1.000)')\n",
    "    print(f'  σ3:          {sv[2]:.6f}  (ideal = 0.000)')\n",
    "    return U, sv, Vt\n",
    "\n",
    "print('\\nEssential Matrix (from F):')\n",
    "U1, sv1, Vt1 = check_essential(E_from_F)\n",
    "\n",
    "print('\\nEssential Matrix (direct 5-point):')\n",
    "U2, sv2, Vt2 = check_essential(E_direct)\n",
    "\n",
    "# Use the directly estimated E going forward\n",
    "E = E_direct\n",
    "e_inliers = e_mask.ravel().astype(bool) if e_mask is not None else np.ones(len(pts1_in), bool)\n",
    "pts1_e = pts1_in[e_inliers]\n",
    "pts2_e = pts2_in[e_inliers]\n",
    "print(f'\\nEssential matrix inliers: {e_inliers.sum()} / {len(pts1_in)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"id": "s7"},
   "source": [
    "## 7. RANSAC — Robust Estimation\n",
    "\n",
    "Both `findFundamentalMat` and `findEssentialMat` internally use **RANSAC (Random Sample Consensus)** to robustly estimate the matrices in the presence of outlier matches.\n",
    "\n",
    "### RANSAC Algorithm\n",
    "\n",
    "```\n",
    "for i = 1 to N_iterations:\n",
    "    1. Sample the minimum number of points (e.g. 8 for F, 5 for E)\n",
    "    2. Fit the model (F or E) to this minimal sample\n",
    "    3. Count inliers: points where error < threshold\n",
    "    4. If inlier count > best so far, save this model\n",
    "\n",
    "Refit the model using all inliers of the best hypothesis\n",
    "```\n",
    "\n",
    "### Number of Iterations Required\n",
    "\n",
    "To guarantee finding a good model with probability $p$ when the outlier ratio is $\\epsilon$ and sample size is $s$:\n",
    "\n",
    "$$N = \\frac{\\log(1 - p)}{\\log(1 - (1 - \\epsilon)^s)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "ransac"},
   "outputs": [],
   "source": [
    "def ransac_iterations(outlier_ratio, sample_size, confidence=0.999):\n",
    "    \"\"\"Compute required RANSAC iterations.\"\"\"\n",
    "    p_all_inliers = (1 - outlier_ratio) ** sample_size\n",
    "    if p_all_inliers >= 1.0:\n",
    "        return 1\n",
    "    return int(np.ceil(np.log(1 - confidence) / np.log(1 - p_all_inliers + 1e-10)))\n",
    "\n",
    "\n",
    "# ── Visualise RANSAC iteration requirements ───────────────────────\n",
    "outlier_ratios  = np.linspace(0.01, 0.90, 100)\n",
    "sample_sizes    = [4, 5, 7, 8]\n",
    "labels          = ['Homography (4pt)', 'Essential E (5pt)', 'F 7-pt', 'F 8-pt']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for s, label in zip(sample_sizes, labels):\n",
    "    iters = [ransac_iterations(e, s) for e in outlier_ratios]\n",
    "    axes[0].semilogy(outlier_ratios * 100, iters, lw=2, label=label)\n",
    "\n",
    "axes[0].set_xlabel('Outlier ratio (%)')\n",
    "axes[0].set_ylabel('Required iterations (log scale)')\n",
    "axes[0].set_title('RANSAC Iterations vs Outlier Ratio\\n(99.9% confidence)', fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "axes[0].set_xlim(0, 90)\n",
    "\n",
    "# ── Inlier/outlier analysis of our match set ─────────────────────\n",
    "all_pts1, all_pts2 = extract_match_points(kps1, kps2, good_matches)\n",
    "\n",
    "# Compute epipolar distances for all matches\n",
    "pts1_h = np.column_stack([all_pts1, np.ones(len(all_pts1))])\n",
    "pts2_h = np.column_stack([all_pts2, np.ones(len(all_pts2))])\n",
    "l2 = (F @ pts1_h.T).T\n",
    "dist_all = np.abs(np.sum(pts2_h * l2, axis=1)) / (np.sqrt(l2[:,0]**2 + l2[:,1]**2) + 1e-10)\n",
    "\n",
    "axes[1].hist(dist_all[dist_all < 50], bins=60, color='steelblue', edgecolor='white', lw=0.3)\n",
    "axes[1].axvline(3.0, color='red', lw=2, ls='--', label='RANSAC threshold = 3px')\n",
    "inlier_count = (dist_all < 3.0).sum()\n",
    "axes[1].set_xlabel('Epipolar distance (pixels)')\n",
    "axes[1].set_ylabel('Match count')\n",
    "axes[1].set_title(f'Epipolar Distance Distribution\\n{inlier_count}/{len(good_matches)} inliers < 3px', fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('RANSAC Analysis', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print iteration table\n",
    "print('RANSAC Iterations Required (99.9% confidence):')\n",
    "print(f'{\"Outlier %\":<12}', end='')\n",
    "for label in labels:\n",
    "    print(f'{label[:12]:<15}', end='')\n",
    "print()\n",
    "print('-' * 72)\n",
    "for e in [0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70]:\n",
    "    print(f'{e*100:>8.0f}%   ', end='')\n",
    "    for s in sample_sizes:\n",
    "        print(f'{ransac_iterations(e,s):<15}', end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"id": "s8"},
   "source": [
    "## 8. Camera Pose Recovery — Decomposing E into R and t\n",
    "\n",
    "Given $\\mathbf{E}$, we recover the relative camera pose $[\\mathbf{R} | \\mathbf{t}]$ via SVD:\n",
    "\n",
    "$$\\mathbf{E} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T$$\n",
    "\n",
    "There are **four candidate solutions** for $(\\mathbf{R}, \\mathbf{t})$:\n",
    "\n",
    "$$\\mathbf{R}_1 = \\mathbf{U}\\mathbf{W}\\mathbf{V}^T, \\quad \\mathbf{R}_2 = \\mathbf{U}\\mathbf{W}^T\\mathbf{V}^T$$\n",
    "$$\\mathbf{t}_1 = +\\mathbf{u}_3, \\quad \\mathbf{t}_2 = -\\mathbf{u}_3$$\n",
    "\n",
    "where $\\mathbf{W} = \\begin{bmatrix} 0 & -1 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}$ and $\\mathbf{u}_3$ is the third column of $\\mathbf{U}$.\n",
    "\n",
    "### Cheirality Check\n",
    "\n",
    "The correct solution is the one where **triangulated 3D points lie in front of both cameras** (positive depth). This is called the **cheirality constraint**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "pose_recovery"},
   "outputs": [],
   "source": [
    "def decompose_essential(E):\n",
    "    \"\"\"\n",
    "    Manually decompose E into four (R, t) candidate solutions.\n",
    "\n",
    "    Returns list of 4 tuples: [(R1,t1), (R1,t2), (R2,t1), (R2,t2)]\n",
    "    \"\"\"\n",
    "    U, sv, Vt = np.linalg.svd(E)\n",
    "\n",
    "    # Enforce E has exactly two equal singular values\n",
    "    s = (sv[0] + sv[1]) / 2.0\n",
    "    E_clean = U @ np.diag([s, s, 0]) @ Vt\n",
    "    U, _, Vt = np.linalg.svd(E_clean)\n",
    "\n",
    "    # Ensure proper rotation (det = +1)\n",
    "    if np.linalg.det(U) < 0: U  *= -1\n",
    "    if np.linalg.det(Vt) < 0: Vt *= -1\n",
    "\n",
    "    W = np.array([[0,-1, 0],\n",
    "                  [1, 0, 0],\n",
    "                  [0, 0, 1]], dtype=np.float64)\n",
    "\n",
    "    R1 = U @ W   @ Vt\n",
    "    R2 = U @ W.T @ Vt\n",
    "    t  = U[:, 2]     # translation (up to sign)\n",
    "\n",
    "    return [(R1,  t), (R1, -t), (R2,  t), (R2, -t)]\n",
    "\n",
    "\n",
    "def triangulate_point(R, t, K, p1, p2):\n",
    "    \"\"\"\n",
    "    Triangulate a single 3D point from two image correspondences.\n",
    "    Uses the Direct Linear Transform (DLT) method.\n",
    "    \"\"\"\n",
    "    P1 = K @ np.hstack([np.eye(3),    np.zeros((3,1))])   # camera 1 at origin\n",
    "    P2 = K @ np.hstack([R, t.reshape(3,1)])                # camera 2\n",
    "\n",
    "    # Build 4×4 system Ax = 0\n",
    "    A = np.array([\n",
    "        p1[1]*P1[2] - P1[1],\n",
    "        P1[0] - p1[0]*P1[2],\n",
    "        p2[1]*P2[2] - P2[1],\n",
    "        P2[0] - p2[0]*P2[2]\n",
    "    ])\n",
    "    _, _, Vt = np.linalg.svd(A)\n",
    "    X = Vt[-1]\n",
    "    return X[:3] / X[3]   # dehomogenise\n",
    "\n",
    "\n",
    "def cheirality_check(R, t, K, pts1, pts2):\n",
    "    \"\"\"\n",
    "    Count how many triangulated points are in front of both cameras.\n",
    "    The correct (R, t) maximises this count.\n",
    "    \"\"\"\n",
    "    P1 = K @ np.hstack([np.eye(3), np.zeros((3,1))])\n",
    "    P2 = K @ np.hstack([R, t.reshape(3,1)])\n",
    "\n",
    "    pts1_h = pts1.T  # (2, N)\n",
    "    pts2_h = pts2.T\n",
    "\n",
    "    pts4d = cv2.triangulatePoints(P1, P2, pts1_h, pts2_h)  # (4, N)\n",
    "    pts3d = (pts4d[:3] / pts4d[3]).T                       # (N, 3)\n",
    "\n",
    "    # Check depth in camera 1 (Z > 0)\n",
    "    depth1 = pts3d[:, 2]\n",
    "\n",
    "    # Check depth in camera 2\n",
    "    X_cam2 = (R @ pts3d.T + t.reshape(3,1)).T\n",
    "    depth2 = X_cam2[:, 2]\n",
    "\n",
    "    n_positive = np.sum((depth1 > 0) & (depth2 > 0))\n",
    "    return n_positive, pts3d\n",
    "\n",
    "\n",
    "# ── Recover camera pose ──────────────────────────────────────────\n",
    "candidates = decompose_essential(E)\n",
    "\n",
    "print('Evaluating 4 candidate (R, t) solutions via cheirality check...')\n",
    "print(f'{\"Solution\":<12} {\"Positive depths\":>18} {\"Selected\":>10}')\n",
    "print('-' * 44)\n",
    "\n",
    "best_count = -1\n",
    "best_R, best_t, best_pts3d = None, None, None\n",
    "\n",
    "# Use subset of inliers for speed\n",
    "n_check = min(200, len(pts1_e))\n",
    "idx_check = np.random.choice(len(pts1_e), n_check, replace=False)\n",
    "\n",
    "for i, (R_c, t_c) in enumerate(candidates):\n",
    "    count, pts3d = cheirality_check(R_c, t_c, K, pts1_e[idx_check], pts2_e[idx_check])\n",
    "    selected = ' ◄ BEST' if count > best_count else ''\n",
    "    print(f'Solution {i+1:<3} {count:>18} {selected}')\n",
    "    if count > best_count:\n",
    "        best_count = count\n",
    "        best_R, best_t = R_c.copy(), t_c.copy()\n",
    "        best_pts3d = pts3d\n",
    "\n",
    "# ── Also use OpenCV's recoverPose (equivalent, more numerically stable)\n",
    "_, R_cv, t_cv, mask_cv = cv2.recoverPose(E, pts1_e, pts2_e, K)\n",
    "\n",
    "print(f'\\nRecovered rotation R (OpenCV):')\n",
    "print(np.round(R_cv, 4))\n",
    "rvec, _ = cv2.Rodrigues(R_cv)\n",
    "angle   = np.linalg.norm(rvec) * 180 / np.pi\n",
    "print(f'Rotation angle: {angle:.2f}°')\n",
    "print(f'\\nRecovered translation t (unit vector):')\n",
    "print(np.round(t_cv.ravel(), 4))\n",
    "\n",
    "R_final = R_cv\n",
    "t_final = t_cv.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"id": "s9"},
   "source": [
    "## 9. Triangulation\n",
    "\n",
    "Given the recovered camera poses $[\\mathbf{R}_1|\\mathbf{t}_1]$ and $[\\mathbf{R}_2|\\mathbf{t}_2]$ and matched 2D points, triangulation recovers the 3D position of each scene point.\n",
    "\n",
    "### Direct Linear Transform (DLT)\n",
    "\n",
    "For each 3D point $\\mathbf{X}$, we have two projection equations (one per camera):\n",
    "\n",
    "$$\\lambda_1 \\mathbf{x}_1 = \\mathbf{P}_1 \\mathbf{X}, \\quad \\lambda_2 \\mathbf{x}_2 = \\mathbf{P}_2 \\mathbf{X}$$\n",
    "\n",
    "Eliminating the scale factors $\\lambda$ gives a homogeneous system $\\mathbf{A}\\mathbf{X} = 0$, solved via SVD:\n",
    "\n",
    "$$\\mathbf{A} = \\begin{bmatrix}\n",
    "x_1 \\mathbf{p}_3^{1T} - \\mathbf{p}_1^{1T} \\\\\n",
    "y_1 \\mathbf{p}_3^{1T} - \\mathbf{p}_2^{1T} \\\\\n",
    "x_2 \\mathbf{p}_3^{2T} - \\mathbf{p}_1^{2T} \\\\\n",
    "y_2 \\mathbf{p}_3^{2T} - \\mathbf{p}_2^{2T}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "The 3D point is the right singular vector corresponding to the smallest singular value.\n",
    "\n",
    "### Triangulation Angle\n",
    "\n",
    "Accuracy improves with a **larger baseline** (angle between viewing rays). Too small an angle (nearly parallel rays) leads to poorly conditioned triangulation and large depth errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "triangulation"},
   "outputs": [],
   "source": [
    "def triangulate_all(R1, t1, R2, t2, K, pts1, pts2):\n",
    "    \"\"\"\n",
    "    Triangulate all matched point pairs using cv2.triangulatePoints.\n",
    "\n",
    "    Camera 1 is placed at world origin: P1 = K [I | 0]\n",
    "    Camera 2 is displaced:              P2 = K [R | t]\n",
    "\n",
    "    Returns:\n",
    "        pts3d : (N, 3) float64 world coordinates\n",
    "        depths: (N,)   depth values in camera 1 frame\n",
    "    \"\"\"\n",
    "    P1 = K @ np.hstack([R1, t1.reshape(3,1)])\n",
    "    P2 = K @ np.hstack([R2, t2.reshape(3,1)])\n",
    "\n",
    "    pts4d = cv2.triangulatePoints(P1, P2, pts1.T, pts2.T)  # (4, N)\n",
    "    pts3d = (pts4d[:3] / pts4d[3]).T                        # (N, 3)\n",
    "    depths = pts4d[2] / pts4d[3]                            # depth in P1 frame\n",
    "\n",
    "    return pts3d, depths\n",
    "\n",
    "\n",
    "def triangulation_angle(pts3d, c1, c2):\n",
    "    \"\"\"\n",
    "    Compute the triangulation angle for each 3D point.\n",
    "    This is the angle at the 3D point between the two rays from c1 and c2.\n",
    "    \"\"\"\n",
    "    r1 = pts3d - c1   # rays from camera 1\n",
    "    r2 = pts3d - c2   # rays from camera 2\n",
    "    cos_angle = np.sum(r1 * r2, axis=1) / (\n",
    "        np.linalg.norm(r1, axis=1) * np.linalg.norm(r2, axis=1) + 1e-10\n",
    "    )\n",
    "    return np.degrees(np.arccos(np.clip(cos_angle, -1, 1)))\n",
    "\n",
    "\n",
    "# Camera 1 at origin\n",
    "R1 = np.eye(3)\n",
    "t1 = np.zeros(3)\n",
    "R2 = R_final\n",
    "t2 = t_final\n",
    "\n",
    "# Triangulate all inlier matches\n",
    "pts3d_all, depths_all = triangulate_all(R1, t1, R2, t2, K, pts1_e, pts2_e)\n",
    "\n",
    "# Filter: keep points in front of both cameras with reasonable depth\n",
    "X_cam2  = (R2 @ pts3d_all.T + t2.reshape(3,1)).T\n",
    "depth2  = X_cam2[:, 2]\n",
    "valid   = (depths_all > 0) & (depth2 > 0) & (depths_all < np.percentile(depths_all[depths_all>0], 98))\n",
    "pts3d   = pts3d_all[valid]\n",
    "\n",
    "print(f'Triangulated {len(pts3d_all)} points, {valid.sum()} pass depth filter')\n",
    "print(f'Depth range  : [{depths_all[valid].min():.3f}, {depths_all[valid].max():.3f}]')\n",
    "\n",
    "# Camera centres\n",
    "c1 = -R1.T @ t1\n",
    "c2 = -R2.T @ t2\n",
    "\n",
    "angles = triangulation_angle(pts3d, c1, c2)\n",
    "print(f'Triangulation angles: mean={angles.mean():.2f}°  median={np.median(angles):.2f}°')\n",
    "\n",
    "# ── Reprojection error ────────────────────────────────────────────\n",
    "def reprojection_error(pts3d, K, R, t, pts2d_obs):\n",
    "    \"\"\"Mean reprojection error in pixels.\"\"\"\n",
    "    proj, _ = project_points(pts3d, K, R, t)\n",
    "    err = np.linalg.norm(proj - pts2d_obs, axis=1)\n",
    "    return err.mean(), err\n",
    "\n",
    "err1_mean, err1 = reprojection_error(pts3d, K, R1, t1, pts1_e[valid])\n",
    "err2_mean, err2 = reprojection_error(pts3d, K, R2, t2, pts2_e[valid])\n",
    "print(f'\\nReprojection error — Camera 1: {err1_mean:.3f} px')\n",
    "print(f'Reprojection error — Camera 2: {err2_mean:.3f} px')\n",
    "\n",
    "# ── Visualise reprojection ────────────────────────────────────────\n",
    "proj1, _ = project_points(pts3d, K, R1, t1)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "img1_rgb = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "axes[0].imshow(img1_rgb)\n",
    "obs = pts1_e[valid]\n",
    "axes[0].scatter(obs[:,0], obs[:,1], c='lime', s=8, label='Observed', zorder=3)\n",
    "axes[0].scatter(proj1[:,0], proj1[:,1], c='red', s=4, marker='+', label=f'Reprojected ({err1_mean:.2f}px)', zorder=4)\n",
    "for o, p in zip(obs[:80], proj1[:80]):\n",
    "    axes[0].plot([o[0],p[0]], [o[1],p[1]], 'y-', lw=0.5, alpha=0.6)\n",
    "axes[0].set_title(f'Reprojection into Camera 1\\nMean error: {err1_mean:.2f} px', fontweight='bold')\n",
    "axes[0].legend(loc='upper right', fontsize=8); axes[0].axis('off')\n",
    "\n",
    "axes[1].hist(np.concatenate([err1, err2]), bins=40, color='steelblue', edgecolor='white', lw=0.5)\n",
    "axes[1].axvline(err1_mean, color='green', lw=2, ls='--', label=f'Cam1 mean: {err1_mean:.2f}px')\n",
    "axes[1].axvline(err2_mean, color='red',   lw=2, ls='--', label=f'Cam2 mean: {err2_mean:.2f}px')\n",
    "axes[1].set_xlabel('Reprojection error (pixels)')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Reprojection Error Distribution', fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('Triangulation Results', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"id": "s10"},
   "source": [
    "## 10. Incremental SfM — More Than Two Views\n",
    "\n",
    "Real SfM extends to **N cameras** incrementally:\n",
    "\n",
    "```\n",
    "1. Initialise with the best two-view pair (largest baseline + most inliers)\n",
    "2. For each new image:\n",
    "   a. Find 2D-3D correspondences (existing 3D points visible in new image)\n",
    "   b. Estimate new camera pose via PnP (Perspective-n-Point)\n",
    "   c. Triangulate new point pairs not yet in the 3D model\n",
    "   d. Run bundle adjustment to reduce accumulated drift\n",
    "3. Repeat until all images are registered\n",
    "```\n",
    "\n",
    "### PnP — Pose from 2D-3D Correspondences\n",
    "\n",
    "Given $N$ known 3D points and their 2D projections in a new view, **PnP** solves for the camera pose $[\\mathbf{R}|\\mathbf{t}]$ directly. OpenCV's `solvePnPRansac` uses the efficient **EPnP** or **iterative** algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "incremental"},
   "outputs": [],
   "source": [
    "def make_multiview_sequence(base_img, n_views=5, seed=1):\n",
    "    \"\"\"\n",
    "    Synthesise a sequence of N views with gradually changing pose.\n",
    "    Returns list of (image, H_world_to_cam) tuples.\n",
    "    \"\"\"\n",
    "    rng   = np.random.default_rng(seed)\n",
    "    views = []\n",
    "    for i in range(n_views):\n",
    "        angle = rng.uniform(-20, 20)\n",
    "        scale = rng.uniform(0.85, 1.1)\n",
    "        tx    = rng.uniform(-60, 60)\n",
    "        ty    = rng.uniform(-40, 40)\n",
    "        img_i, _, H = make_view_pair(base_img, angle_deg=angle,\n",
    "                                       scale=scale, tx=int(tx), ty=int(ty))\n",
    "        views.append((img_i, H))\n",
    "    return views\n",
    "\n",
    "\n",
    "class IncrementalSfM:\n",
    "    \"\"\"\n",
    "    Minimal incremental SfM pipeline for educational purposes.\n",
    "    Handles: feature detection, pairwise matching, essential matrix,\n",
    "    triangulation, PnP registration, and iterative refinement.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, K, n_features=1500, ratio_thresh=0.75):\n",
    "        self.K             = K\n",
    "        self.n_features    = n_features\n",
    "        self.ratio_thresh  = ratio_thresh\n",
    "\n",
    "        # State\n",
    "        self.images        = []\n",
    "        self.all_kps       = []\n",
    "        self.all_descs     = []\n",
    "        self.all_pts       = []\n",
    "        self.camera_poses  = {}    # {img_idx: (R, t)}\n",
    "        self.points3d      = []    # list of (X, Y, Z)\n",
    "        self.point_colors  = []    # list of (R, G, B)\n",
    "        self.observations  = {}    # {pt3d_idx: [(img_idx, kp_idx), ...]}\n",
    "\n",
    "    def add_image(self, img):\n",
    "        \"\"\"Register an image and detect its features.\"\"\"\n",
    "        kps, descs, pts = detect_features(img, self.n_features)\n",
    "        idx = len(self.images)\n",
    "        self.images.append(img)\n",
    "        self.all_kps.append(kps)\n",
    "        self.all_descs.append(descs)\n",
    "        self.all_pts.append(pts)\n",
    "        return idx\n",
    "\n",
    "    def match_pair(self, i, j):\n",
    "        \"\"\"Match features between images i and j.\"\"\"\n",
    "        matches, _ = match_features(\n",
    "            self.all_descs[i], self.all_descs[j], self.ratio_thresh\n",
    "        )\n",
    "        if len(matches) < 8:\n",
    "            return None, None\n",
    "        pts_i = np.float32([self.all_kps[i][m.queryIdx].pt for m in matches])\n",
    "        pts_j = np.float32([self.all_kps[j][m.trainIdx].pt for m in matches])\n",
    "        return pts_i, pts_j\n",
    "\n",
    "    def initialise(self, i=0, j=1):\n",
    "        \"\"\"Bootstrap the reconstruction from a two-view pair.\"\"\"\n",
    "        p_i, p_j = self.match_pair(i, j)\n",
    "        if p_i is None:\n",
    "            print('Initialisation failed: not enough matches')\n",
    "            return False\n",
    "\n",
    "        E, mask = cv2.findEssentialMat(p_i, p_j, self.K,\n",
    "                                        method=cv2.RANSAC, prob=0.999, threshold=1.0)\n",
    "        if E is None: return False\n",
    "\n",
    "        inliers = mask.ravel().astype(bool)\n",
    "        _, R, t, pose_mask = cv2.recoverPose(E, p_i[inliers], p_j[inliers], self.K)\n",
    "\n",
    "        R1, t1 = np.eye(3), np.zeros(3)\n",
    "        R2, t2 = R, t.ravel()\n",
    "\n",
    "        self.camera_poses[i] = (R1, t1)\n",
    "        self.camera_poses[j] = (R2, t2)\n",
    "\n",
    "        pts3d, depths = triangulate_all(R1, t1, R2, t2, self.K,\n",
    "                                         p_i[inliers], p_j[inliers])\n",
    "\n",
    "        valid = (depths > 0) & (depths < np.percentile(depths[depths>0]+1e-6, 99))\n",
    "        self.points3d   = list(pts3d[valid])\n",
    "\n",
    "        # Sample colors from image 1\n",
    "        img_rgb = cv2.cvtColor(self.images[i], cv2.COLOR_BGR2RGB)\n",
    "        for pt in p_i[inliers][valid]:\n",
    "            x, y = int(np.clip(pt[0], 0, img_rgb.shape[1]-1)), \\\n",
    "                   int(np.clip(pt[1], 0, img_rgb.shape[0]-1))\n",
    "            self.point_colors.append(img_rgb[y, x] / 255.0)\n",
    "\n",
    "        print(f'Initialised with images {i}+{j}: {len(self.points3d)} 3D points')\n",
    "        return True\n",
    "\n",
    "    def register_image(self, new_idx, ref_idx):\n",
    "        \"\"\"Register a new image using PnP against the existing 3D model.\"\"\"\n",
    "        if new_idx in self.camera_poses:\n",
    "            return True   # already registered\n",
    "\n",
    "        p_ref, p_new = self.match_pair(ref_idx, new_idx)\n",
    "        if p_ref is None or len(self.points3d) < 6:\n",
    "            print(f'  Image {new_idx}: skipping — insufficient matches or 3D points')\n",
    "            return False\n",
    "\n",
    "        # For this simplified demo, we use the triangulated points\n",
    "        # and find correspondences via nearest-neighbour matching in 2D\n",
    "        n_pts = min(len(self.points3d), 300)\n",
    "        pts3d_arr = np.array(self.points3d[:n_pts], dtype=np.float32)\n",
    "\n",
    "        R_ref, t_ref = self.camera_poses[ref_idx]\n",
    "        proj_ref, depths_ref = project_points(\n",
    "            pts3d_arr, self.K, R_ref, t_ref\n",
    "        )\n",
    "\n",
    "        # Find 2D-3D correspondences via proximity\n",
    "        valid_proj = (depths_ref > 0) & \\\n",
    "                     (proj_ref[:,0] >= 0) & (proj_ref[:,0] < self.images[ref_idx].shape[1]) & \\\n",
    "                     (proj_ref[:,1] >= 0) & (proj_ref[:,1] < self.images[ref_idx].shape[0])\n",
    "\n",
    "        if valid_proj.sum() < 6:\n",
    "            return False\n",
    "\n",
    "        pts3d_valid  = pts3d_arr[valid_proj]\n",
    "        proj_valid   = proj_ref[valid_proj]\n",
    "\n",
    "        # Match: for each 3D point projected into ref, find closest match in new image\n",
    "        # Use FLANN on the matched region\n",
    "        correspondences_3d, correspondences_2d = [], []\n",
    "        for k, (px_ref, pt3) in enumerate(zip(proj_valid, pts3d_valid)):\n",
    "            dists = np.linalg.norm(p_ref - px_ref, axis=1)\n",
    "            idx_near = np.argmin(dists)\n",
    "            if dists[idx_near] < 5.0:\n",
    "                correspondences_3d.append(pt3)\n",
    "                correspondences_2d.append(p_new[idx_near])\n",
    "\n",
    "        if len(correspondences_3d) < 6:\n",
    "            print(f'  Image {new_idx}: only {len(correspondences_3d)} 2D-3D correspondences')\n",
    "            return False\n",
    "\n",
    "        pts3d_pnp = np.array(correspondences_3d, dtype=np.float32)\n",
    "        pts2d_pnp = np.array(correspondences_2d, dtype=np.float32)\n",
    "\n",
    "        success, rvec, tvec, pnp_inliers = cv2.solvePnPRansac(\n",
    "            pts3d_pnp, pts2d_pnp, self.K, None,\n",
    "            confidence=0.99, reprojectionError=4.0,\n",
    "            flags=cv2.SOLVEPNP_EPNP\n",
    "        )\n",
    "\n",
    "        if not success or pnp_inliers is None:\n",
    "            print(f'  Image {new_idx}: PnP failed')\n",
    "            return False\n",
    "\n",
    "        R_new, _ = cv2.Rodrigues(rvec)\n",
    "        t_new    = tvec.ravel()\n",
    "        self.camera_poses[new_idx] = (R_new, t_new)\n",
    "\n",
    "        # Triangulate new points\n",
    "        R_ref_cam, t_ref_cam = self.camera_poses[ref_idx]\n",
    "        pts3d_new, depths_new = triangulate_all(\n",
    "            R_ref_cam, t_ref_cam, R_new, t_new,\n",
    "            self.K, p_ref, p_new\n",
    "        )\n",
    "\n",
    "        img_rgb = cv2.cvtColor(self.images[new_idx], cv2.COLOR_BGR2RGB)\n",
    "        n_added = 0\n",
    "        for k, (pt, d) in enumerate(zip(pts3d_new, depths_new)):\n",
    "            if d > 0 and d < 200:\n",
    "                self.points3d.append(pt)\n",
    "                px = p_new[k]\n",
    "                x  = int(np.clip(px[0], 0, img_rgb.shape[1]-1))\n",
    "                y  = int(np.clip(px[1], 0, img_rgb.shape[0]-1))\n",
    "                self.point_colors.append(img_rgb[y, x] / 255.0)\n",
    "                n_added += 1\n",
    "\n",
    "        print(f'  Image {new_idx}: pose recovered, +{n_added} new 3D points '\n",
    "              f'(total: {len(self.points3d)})')\n",
    "        return True\n",
    "\n",
    "\n",
    "# ── Run the pipeline on a synthetic sequence ─────────────────────\n",
    "print('Building synthetic multi-view sequence...')\n",
    "views = make_multiview_sequence(base_img, n_views=5, seed=42)\n",
    "\n",
    "sfm = IncrementalSfM(K, n_features=1500)\n",
    "\n",
    "for img_view, _ in views:\n",
    "    sfm.add_image(img_view)\n",
    "\n",
    "print('\\nRunning incremental SfM...')\n",
    "sfm.initialise(0, 1)\n",
    "for i in range(2, len(views)):\n",
    "    sfm.register_image(i, i-1)\n",
    "\n",
    "print(f'\\nFinal reconstruction:')\n",
    "print(f'  Registered cameras : {len(sfm.camera_poses)}')\n",
    "print(f'  3D points          : {len(sfm.points3d)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"id": "s11"},
   "source": [
    "## 11. Bundle Adjustment\n",
    "\n",
    "After incremental registration, small errors in each step **accumulate** (drift). Bundle Adjustment (BA) is the global non-linear optimisation that simultaneously refines all camera poses and 3D point positions by minimising the total **reprojection error**:\n",
    "\n",
    "$$\\min_{\\{\\mathbf{R}_i, \\mathbf{t}_i, \\mathbf{X}_j\\}} \\sum_{i,j} \\rho\\left(\\left\\| \\mathbf{x}_{ij} - \\pi(\\mathbf{K}, \\mathbf{R}_i, \\mathbf{t}_i, \\mathbf{X}_j) \\right\\|^2\\right)$$\n",
    "\n",
    "where:\n",
    "- $\\mathbf{x}_{ij}$ is the **observed** 2D location of point $j$ in image $i$\n",
    "- $\\pi(\\cdot)$ is the **projection** function\n",
    "- $\\rho(\\cdot)$ is a **robust loss** (Huber/Cauchy) to down-weight outliers\n",
    "\n",
    "### Parameterisation\n",
    "\n",
    "| Variable | Representation | DOF |\n",
    "|---|---|---|\n",
    "| Rotation $\\mathbf{R}_i$ | Rodrigues vector (axis-angle) | 3 |\n",
    "| Translation $\\mathbf{t}_i$ | 3D vector | 3 |\n",
    "| 3D point $\\mathbf{X}_j$ | 3D Euclidean | 3 |\n",
    "\n",
    "### Solving with Levenberg-Marquardt\n",
    "\n",
    "The Jacobian is **sparse** — each 3D point only appears in a small subset of images. The **Schur complement trick** exploits this structure to solve the normal equations efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "bundle_adjustment"},
   "outputs": [],
   "source": [
    "def rodrigues_to_R(rvec):\n",
    "    \"\"\"Convert Rodrigues (axis-angle) vector to rotation matrix.\"\"\"\n",
    "    R, _ = cv2.Rodrigues(rvec.reshape(3,1))\n",
    "    return R\n",
    "\n",
    "\n",
    "def R_to_rodrigues(R):\n",
    "    \"\"\"Convert rotation matrix to Rodrigues vector.\"\"\"\n",
    "    rvec, _ = cv2.Rodrigues(R)\n",
    "    return rvec.ravel()\n",
    "\n",
    "\n",
    "def pack_params(camera_poses, points3d):\n",
    "    \"\"\"\n",
    "    Pack all optimisation variables into a 1D parameter vector.\n",
    "    Format: [rvec0, t0, rvec1, t1, ..., X0, X1, ...]\n",
    "    Each camera: 6 params (3 rotation + 3 translation)\n",
    "    Each point : 3 params\n",
    "    \"\"\"\n",
    "    params = []\n",
    "    for R, t in camera_poses:\n",
    "        params.extend(R_to_rodrigues(R))   # 3\n",
    "        params.extend(t)                   # 3\n",
    "    for pt in points3d:\n",
    "        params.extend(pt)                  # 3\n",
    "    return np.array(params, dtype=np.float64)\n",
    "\n",
    "\n",
    "def unpack_params(params, n_cameras, n_points):\n",
    "    \"\"\"Unpack 1D parameter vector back into camera poses and 3D points.\"\"\"\n",
    "    cameras = []\n",
    "    for i in range(n_cameras):\n",
    "        rvec = params[i*6 : i*6+3]\n",
    "        t    = params[i*6+3 : i*6+6]\n",
    "        cameras.append((rodrigues_to_R(rvec), t))\n",
    "\n",
    "    start = n_cameras * 6\n",
    "    pts3d = params[start:].reshape(n_points, 3)\n",
    "    return cameras, pts3d\n",
    "\n",
    "\n",
    "def reprojection_residuals(params, K, n_cameras, n_points,\n",
    "                            cam_indices, pt_indices, observed_pts):\n",
    "    \"\"\"\n",
    "    Residual function for bundle adjustment.\n",
    "    Returns flattened (u_proj - u_obs, v_proj - v_obs) for all observations.\n",
    "    \"\"\"\n",
    "    cameras, pts3d = unpack_params(params, n_cameras, n_points)\n",
    "\n",
    "    residuals = []\n",
    "    for cam_idx, pt_idx, obs in zip(cam_indices, pt_indices, observed_pts):\n",
    "        R, t = cameras[cam_idx]\n",
    "        X = pts3d[pt_idx]\n",
    "\n",
    "        # Project X\n",
    "        X_cam = R @ X + t\n",
    "        if abs(X_cam[2]) < 1e-6:\n",
    "            residuals.extend([0.0, 0.0])\n",
    "            continue\n",
    "        u = K[0,0] * X_cam[0] / X_cam[2] + K[0,2]\n",
    "        v = K[1,1] * X_cam[1] / X_cam[2] + K[1,2]\n",
    "        residuals.extend([u - obs[0], v - obs[1]])\n",
    "\n",
    "    return np.array(residuals)\n",
    "\n",
    "\n",
    "def bundle_adjustment(K, camera_poses_list, pts3d_arr,\n",
    "                       cam_indices, pt_indices, observed_pts,\n",
    "                       max_nfev=200, verbose=0):\n",
    "    \"\"\"\n",
    "    Run sparse bundle adjustment using scipy least_squares.\n",
    "\n",
    "    Args:\n",
    "        camera_poses_list : list of (R, t) for each registered camera\n",
    "        pts3d_arr         : (M, 3) initial 3D point positions\n",
    "        cam_indices       : (N,) camera index for each observation\n",
    "        pt_indices        : (N,) 3D point index for each observation\n",
    "        observed_pts      : (N, 2) observed 2D positions\n",
    "\n",
    "    Returns:\n",
    "        cameras_opt : list of refined (R, t)\n",
    "        pts3d_opt   : (M, 3) refined 3D positions\n",
    "        result      : scipy OptimizeResult\n",
    "    \"\"\"\n",
    "    n_cameras = len(camera_poses_list)\n",
    "    n_points  = len(pts3d_arr)\n",
    "\n",
    "    x0 = pack_params(camera_poses_list, pts3d_arr)\n",
    "\n",
    "    # Sparse Jacobian structure\n",
    "    n_obs = len(cam_indices)\n",
    "    A = lil_matrix((2 * n_obs, 6*n_cameras + 3*n_points), dtype=int)\n",
    "    for k, (ci, pi) in enumerate(zip(cam_indices, pt_indices)):\n",
    "        A[2*k,   ci*6   : ci*6+6] = 1\n",
    "        A[2*k+1, ci*6   : ci*6+6] = 1\n",
    "        A[2*k,   6*n_cameras + pi*3 : 6*n_cameras + pi*3+3] = 1\n",
    "        A[2*k+1, 6*n_cameras + pi*3 : 6*n_cameras + pi*3+3] = 1\n",
    "\n",
    "    result = least_squares(\n",
    "        reprojection_residuals, x0,\n",
    "        jac_sparsity=A,\n",
    "        method='trf',\n",
    "        loss='huber',             # robust loss\n",
    "        f_scale=2.0,              # Huber threshold in pixels\n",
    "        max_nfev=max_nfev,\n",
    "        verbose=verbose,\n",
    "        args=(K, n_cameras, n_points, cam_indices, pt_indices, observed_pts)\n",
    "    )\n",
    "\n",
    "    cameras_opt, pts3d_opt = unpack_params(result.x, n_cameras, n_points)\n",
    "    return cameras_opt, pts3d_opt, result\n",
    "\n",
    "\n",
    "# ── Run BA on the two-camera case ────────────────────────────────\n",
    "print('Running Bundle Adjustment...')\n",
    "\n",
    "# Build observation lists from the inlier matches\n",
    "n_pts_ba = min(len(pts3d), 300)\n",
    "pts3d_ba = np.array(pts3d[:n_pts_ba])\n",
    "obs1_ba  = pts1_e[valid][:n_pts_ba]\n",
    "obs2_ba  = pts2_e[valid][:n_pts_ba]\n",
    "\n",
    "cam_idx  = np.concatenate([np.zeros(n_pts_ba, int), np.ones(n_pts_ba, int)])\n",
    "pt_idx   = np.concatenate([np.arange(n_pts_ba), np.arange(n_pts_ba)])\n",
    "obs_all  = np.vstack([obs1_ba, obs2_ba])\n",
    "\n",
    "poses_before = [(R1, t1), (R_final, t_final)]\n",
    "\n",
    "# Compute reprojection error before BA\n",
    "r_before = reprojection_residuals(\n",
    "    pack_params(poses_before, pts3d_ba),\n",
    "    K, 2, n_pts_ba, cam_idx, pt_idx, obs_all\n",
    ")\n",
    "err_before = np.sqrt(np.mean(r_before.reshape(-1,2)**2, axis=1)).mean()\n",
    "\n",
    "cameras_opt, pts3d_opt, ba_result = bundle_adjustment(\n",
    "    K, poses_before, pts3d_ba,\n",
    "    cam_idx, pt_idx, obs_all,\n",
    "    max_nfev=100, verbose=0\n",
    ")\n",
    "\n",
    "# Compute reprojection error after BA\n",
    "r_after = reprojection_residuals(\n",
    "    pack_params(cameras_opt, pts3d_opt),\n",
    "    K, 2, n_pts_ba, cam_idx, pt_idx, obs_all\n",
    ")\n",
    "err_after = np.sqrt(np.mean(r_after.reshape(-1,2)**2, axis=1)).mean()\n",
    "\n",
    "print(f'Mean reprojection error — before BA : {err_before:.4f} px')\n",
    "print(f'Mean reprojection error — after  BA : {err_after:.4f} px')\n",
    "print(f'Improvement: {100*(err_before - err_after)/err_before:.1f}%')\n",
    "print(f'BA iterations: {ba_result.nfev}')\n",
    "print(f'Termination  : {ba_result.message}')\n",
    "\n",
    "# Plot convergence\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 4))\n",
    "\n",
    "res_before = np.sqrt(r_before.reshape(-1,2)[:,0]**2 + r_before.reshape(-1,2)[:,1]**2)\n",
    "res_after  = np.sqrt(r_after.reshape(-1,2)[:,0]**2  + r_after.reshape(-1,2)[:,1]**2)\n",
    "\n",
    "axes[0].hist(res_before, bins=40, alpha=0.6, color='red',  label=f'Before BA (μ={err_before:.2f}px)')\n",
    "axes[0].hist(res_after,  bins=40, alpha=0.6, color='blue', label=f'After  BA (μ={err_after:.2f}px)')\n",
    "axes[0].set_xlabel('Reprojection error (px)')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Bundle Adjustment — Error Distribution', fontweight='bold')\n",
    "axes[0].legend(); axes[0].grid(alpha=0.3)\n",
    "\n",
    "shift3d = np.linalg.norm(pts3d_opt - pts3d_ba, axis=1)\n",
    "axes[1].hist(shift3d, bins=40, color='steelblue', edgecolor='white', lw=0.5)\n",
    "axes[1].set_xlabel('3D point shift magnitude')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title(f'BA 3D Point Adjustments\\nMean shift: {shift3d.mean():.4f}', fontweight='bold')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"id": "s12"},
   "source": [
    "## 12. 3D Point Cloud Visualization\n",
    "\n",
    "We now combine everything to visualise the final sparse reconstruction: the 3D point cloud and the recovered camera poses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "visualisation"},
   "outputs": [],
   "source": [
    "def draw_camera_frustum(fig, R, t, K, H=480, W=640, scale=0.3, color='blue', name='Camera'):\n",
    "    \"\"\"\n",
    "    Add a camera frustum to a Plotly 3D figure.\n",
    "    The frustum shows the camera's position and orientation in 3D.\n",
    "    \"\"\"\n",
    "    # Camera centre in world coords\n",
    "    C = -R.T @ t\n",
    "\n",
    "    # Four corners of the image plane (in camera coordinates)\n",
    "    corners_cam = np.array([\n",
    "        [(0 - K[0,2]) / K[0,0], (0 - K[1,2]) / K[1,1], 1],\n",
    "        [(W - K[0,2]) / K[0,0], (0 - K[1,2]) / K[1,1], 1],\n",
    "        [(W - K[0,2]) / K[0,0], (H - K[1,2]) / K[1,1], 1],\n",
    "        [(0 - K[0,2]) / K[0,0], (H - K[1,2]) / K[1,1], 1],\n",
    "    ]) * scale\n",
    "\n",
    "    # Transform to world\n",
    "    corners_world = (R.T @ corners_cam.T - R.T @ t.reshape(3,1)).T\n",
    "\n",
    "    # Edges of frustum\n",
    "    edges = []\n",
    "    for corner in corners_world:\n",
    "        edges += [C.tolist(), corner.tolist(), [None,None,None]]\n",
    "    for i in range(4):\n",
    "        edges += [corners_world[i].tolist(), corners_world[(i+1)%4].tolist(), [None,None,None]]\n",
    "\n",
    "    ex = [e[0] for e in edges]\n",
    "    ey = [e[1] for e in edges]\n",
    "    ez = [e[2] for e in edges]\n",
    "\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=ex, y=ey, z=ez,\n",
    "        mode='lines',\n",
    "        line=dict(color=color, width=2),\n",
    "        name=name,\n",
    "        showlegend=True\n",
    "    ))\n",
    "\n",
    "    # Camera centre marker\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=[C[0]], y=[C[1]], z=[C[2]],\n",
    "        mode='markers',\n",
    "        marker=dict(size=6, color=color),\n",
    "        name=f'{name} centre',\n",
    "        showlegend=False\n",
    "    ))\n",
    "\n",
    "\n",
    "# ── Interactive Plotly 3D visualisation ───────────────────────────\n",
    "pts3d_vis   = np.array(pts3d_opt if len(pts3d_opt) > 0 else pts3d)\n",
    "colors_vis  = np.array(sfm.point_colors[:len(pts3d_vis)]) if sfm.point_colors else None\n",
    "\n",
    "# Remove outliers using IQR\n",
    "def remove_outliers_iqr(pts, factor=3.0):\n",
    "    med = np.median(pts, axis=0)\n",
    "    mad = np.median(np.abs(pts - med), axis=0) + 1e-6\n",
    "    mask = np.all(np.abs(pts - med) < factor * mad * 1.4826, axis=1)\n",
    "    return mask\n",
    "\n",
    "mask_ok     = remove_outliers_iqr(pts3d_vis)\n",
    "pts3d_clean = pts3d_vis[mask_ok]\n",
    "\n",
    "if colors_vis is not None and len(colors_vis) == len(mask_ok):\n",
    "    colors_clean = colors_vis[mask_ok]\n",
    "    color_strs = [f'rgb({int(c[0]*255)},{int(c[1]*255)},{int(c[2]*255)})'\n",
    "                  for c in colors_clean]\n",
    "else:\n",
    "    depths_clean = pts3d_clean[:, 2]\n",
    "    d_norm = (depths_clean - depths_clean.min()) / (depths_clean.ptp() + 1e-6)\n",
    "    cmap   = plt.cm.plasma\n",
    "    color_strs = [f'rgb({int(c[0]*255)},{int(c[1]*255)},{int(c[2]*255)})'\n",
    "                  for c in cmap(d_norm)[:, :3]]\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Point cloud\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=pts3d_clean[:,0],\n",
    "    y=pts3d_clean[:,1],\n",
    "    z=pts3d_clean[:,2],\n",
    "    mode='markers',\n",
    "    marker=dict(size=2, color=color_strs, opacity=0.8),\n",
    "    name=f'3D Points ({len(pts3d_clean)})'\n",
    "))\n",
    "\n",
    "# Camera frustums\n",
    "cam_colors = ['#e74c3c', '#3498db', '#2ecc71', '#f39c12', '#9b59b6']\n",
    "for i, (R_c, t_c) in enumerate(cameras_opt):\n",
    "    draw_camera_frustum(fig, R_c, t_c, K, scale=0.4,\n",
    "                         color=cam_colors[i % len(cam_colors)],\n",
    "                         name=f'Camera {i}')\n",
    "\n",
    "# Also show SfM cameras if registered\n",
    "for cam_idx, (R_c, t_c) in list(sfm.camera_poses.items())[2:]:\n",
    "    draw_camera_frustum(fig, R_c, t_c, K, scale=0.4,\n",
    "                         color=cam_colors[cam_idx % len(cam_colors)],\n",
    "                         name=f'SfM Camera {cam_idx}')\n",
    "\n",
    "fig.update_layout(\n",
    "    title=dict(text='Structure from Motion — 3D Reconstruction', font=dict(size=15)),\n",
    "    scene=dict(\n",
    "        xaxis_title='X', yaxis_title='Y', zaxis_title='Z (depth)',\n",
    "        aspectmode='data',\n",
    "        camera=dict(eye=dict(x=0, y=-2, z=0.5))\n",
    "    ),\n",
    "    height=700,\n",
    "    legend=dict(x=0.01, y=0.99)\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(f'\\n3D Reconstruction Summary:')\n",
    "print(f'  Points in cloud      : {len(pts3d_clean)}')\n",
    "print(f'  Cameras reconstructed: {len(cameras_opt)}')\n",
    "print(f'  Point cloud extent   : X=[{pts3d_clean[:,0].min():.2f}, {pts3d_clean[:,0].max():.2f}]')\n",
    "print(f'                         Y=[{pts3d_clean[:,1].min():.2f}, {pts3d_clean[:,1].max():.2f}]')\n",
    "print(f'                         Z=[{pts3d_clean[:,2].min():.2f}, {pts3d_clean[:,2].max():.2f}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "matplotlib_vis"},
   "outputs": [],
   "source": [
    "# ── Static matplotlib summary figure ─────────────────────────────\n",
    "\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "gs  = gridspec.GridSpec(2, 3, figure=fig)\n",
    "\n",
    "# (0,0) — Image 1 with keypoints\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax1.imshow(visualise_keypoints(img1, kps1, max_show=300))\n",
    "ax1.set_title(f'1. SIFT Keypoints\\n({len(kps1)} detected)', fontweight='bold'); ax1.axis('off')\n",
    "\n",
    "# (0,1) — Feature matches\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax2.imshow(draw_matches_vis(img1, kps1, img2, kps2, good_matches[:60]))\n",
    "ax2.set_title(f'2. Feature Matches\\n({len(good_matches)} after ratio test)', fontweight='bold'); ax2.axis('off')\n",
    "\n",
    "# (0,2) — Epipolar lines\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "ax3.imshow(epi2)\n",
    "ax3.set_title('3. Epipolar Lines\\n(Fundamental Matrix)', fontweight='bold'); ax3.axis('off')\n",
    "\n",
    "# (1,0) — Reprojection\n",
    "ax4 = fig.add_subplot(gs[1, 0])\n",
    "ax4.imshow(cv2.cvtColor(img1, cv2.COLOR_BGR2RGB))\n",
    "ax4.scatter(obs1_ba[:,0], obs1_ba[:,1], c='lime', s=5, label='Observed')\n",
    "proj_opt, _ = project_points(pts3d_opt, K, *cameras_opt[0])\n",
    "ax4.scatter(proj_opt[:,0], proj_opt[:,1], c='red', s=3, marker='+', label=f'Reprojected')\n",
    "ax4.set_title(f'4. Reprojection after BA\\n(error={err_after:.2f}px)', fontweight='bold')\n",
    "ax4.legend(fontsize=7); ax4.axis('off')\n",
    "\n",
    "# (1,1) — 3D point cloud (top view)\n",
    "ax5 = fig.add_subplot(gs[1, 1])\n",
    "sc = ax5.scatter(pts3d_clean[:,0], pts3d_clean[:,2],\n",
    "                  c=pts3d_clean[:,1], cmap='plasma', s=2, alpha=0.6)\n",
    "for i, (R_c, t_c) in enumerate(cameras_opt):\n",
    "    C = -R_c.T @ t_c\n",
    "    ax5.scatter(C[0], C[2], s=120, marker='^',\n",
    "                color=cam_colors[i % len(cam_colors)],\n",
    "                zorder=5, label=f'Cam {i}')\n",
    "plt.colorbar(sc, ax=ax5, label='Y height')\n",
    "ax5.set_xlabel('X'); ax5.set_ylabel('Z (depth)')\n",
    "ax5.set_title('5. 3D Point Cloud (top view)', fontweight='bold')\n",
    "ax5.legend(fontsize=7); ax5.grid(alpha=0.3)\n",
    "\n",
    "# (1,2) — 3D side view\n",
    "ax6 = fig.add_subplot(gs[1, 2], projection='3d')\n",
    "ax6.scatter(pts3d_clean[::3,0], pts3d_clean[::3,1], pts3d_clean[::3,2],\n",
    "             c=pts3d_clean[::3,2], cmap='plasma', s=1, alpha=0.5)\n",
    "for i, (R_c, t_c) in enumerate(cameras_opt):\n",
    "    C = -R_c.T @ t_c\n",
    "    ax6.scatter(*C, s=80, marker='^', color=cam_colors[i % len(cam_colors)], zorder=5)\n",
    "ax6.set_xlabel('X'); ax6.set_ylabel('Y'); ax6.set_zlabel('Z')\n",
    "ax6.set_title('6. 3D Point Cloud (3D view)', fontweight='bold')\n",
    "\n",
    "plt.suptitle('Structure from Motion — Complete Pipeline Summary', fontsize=15, fontweight='bold', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.savefig('/content/sfm_summary.png', dpi=130, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('Summary figure saved to /content/sfm_summary.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"id": "s13"},
   "source": [
    "## 13. Upload Your Own Images\n",
    "\n",
    "Run the full SfM pipeline on your own photo sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {"id": "user_images"},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "print('Upload 3 or more images of the SAME scene from DIFFERENT viewpoints.')\n",
    "print('Tips:')\n",
    "print('  - Move the camera sideways between shots (good baseline)')\n",
    "print('  - Keep ~60-80% overlap between consecutive images')\n",
    "print('  - Avoid motion blur and low texture scenes')\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "if len(uploaded) >= 2:\n",
    "    user_imgs = []\n",
    "    for fname in sorted(uploaded.keys()):\n",
    "        img_u = cv2.imread(fname)\n",
    "        if img_u is None:\n",
    "            img_u = cv2.imdecode(np.frombuffer(uploaded[fname], np.uint8), cv2.IMREAD_COLOR)\n",
    "        # Resize to manageable resolution\n",
    "        h_u, w_u = img_u.shape[:2]\n",
    "        scale_u  = min(1.0, 800 / max(h_u, w_u))\n",
    "        img_u    = cv2.resize(img_u, (int(w_u*scale_u), int(h_u*scale_u)))\n",
    "        user_imgs.append(img_u)\n",
    "        print(f'  Loaded: {fname}  {img_u.shape[:2]}')\n",
    "\n",
    "    # Estimate K from EXIF focal length or use a reasonable default\n",
    "    H_u, W_u = user_imgs[0].shape[:2]\n",
    "    K_user   = build_K(fx=max(H_u,W_u)*1.2, fy=max(H_u,W_u)*1.2, cx=W_u/2, cy=H_u/2)\n",
    "    print(f'\\nUsing estimated K (focal={max(H_u,W_u)*1.2:.0f}px):')\n",
    "    print(K_user)\n",
    "\n",
    "    # Run SfM\n",
    "    sfm_user = IncrementalSfM(K_user, n_features=2000)\n",
    "    for img_u in user_imgs:\n",
    "        sfm_user.add_image(img_u)\n",
    "\n",
    "    success = sfm_user.initialise(0, 1)\n",
    "    if success:\n",
    "        for i in range(2, len(user_imgs)):\n",
    "            sfm_user.register_image(i, i-1)\n",
    "\n",
    "        # Visualise\n",
    "        pts_u = np.array(sfm_user.points3d)\n",
    "        if len(pts_u) > 3:\n",
    "            mask_u = remove_outliers_iqr(pts_u, factor=4.0)\n",
    "            pts_u_clean = pts_u[mask_u]\n",
    "\n",
    "            fig_u = go.Figure()\n",
    "            colors_u = [f'rgb({int(c[0]*255)},{int(c[1]*255)},{int(c[2]*255)})'\n",
    "                        for c in (np.array(sfm_user.point_colors)[mask_u]\n",
    "                                  if len(sfm_user.point_colors) == len(pts_u)\n",
    "                                  else np.ones((len(pts_u_clean), 3)) * 0.5)]\n",
    "\n",
    "            fig_u.add_trace(go.Scatter3d(\n",
    "                x=pts_u_clean[:,0], y=pts_u_clean[:,1], z=pts_u_clean[:,2],\n",
    "                mode='markers',\n",
    "                marker=dict(size=2, color=colors_u, opacity=0.9),\n",
    "                name=f'3D points ({len(pts_u_clean)})'\n",
    "            ))\n",
    "            for i, (R_u, t_u) in sfm_user.camera_poses.items():\n",
    "                draw_camera_frustum(fig_u, R_u, t_u, K_user, scale=0.5,\n",
    "                                     color=cam_colors[i % len(cam_colors)],\n",
    "                                     name=f'Camera {i}')\n",
    "\n",
    "            fig_u.update_layout(\n",
    "                title='SfM — Your Images',\n",
    "                scene=dict(aspectmode='data'),\n",
    "                height=650\n",
    "            )\n",
    "            fig_u.show()\n",
    "    else:\n",
    "        print('Initialisation failed. Try images with more overlap and texture.')\n",
    "else:\n",
    "    print('Upload at least 2 images to run SfM on your own data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {"id": "summary"},
   "source": [
    "## Summary — The Full SfM Pipeline\n",
    "\n",
    "| Step | Method | Key Equation / Tool |\n",
    "|---|---|---|\n",
    "| **Camera model** | Pinhole projection | $\\lambda\\mathbf{x} = \\mathbf{K}[\\mathbf{R}|\\mathbf{t}]\\mathbf{X}$ |\n",
    "| **Feature detection** | SIFT | DoG scale-space extrema |\n",
    "| **Feature matching** | FLANN + ratio test | $d_1/d_2 < 0.75$ |\n",
    "| **Fundamental matrix** | 8-point + RANSAC | $\\mathbf{x}'^T\\mathbf{F}\\mathbf{x} = 0$ |\n",
    "| **Essential matrix** | 5-point algorithm | $\\mathbf{E} = \\mathbf{K}'^T\\mathbf{F}\\mathbf{K}$ |\n",
    "| **Pose recovery** | SVD + cheirality | $\\mathbf{E} = \\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^T \\rightarrow (\\mathbf{R}, \\mathbf{t})$ |\n",
    "| **Triangulation** | DLT | Solve $\\mathbf{A}\\mathbf{X}=0$ via SVD |\n",
    "| **New view registration** | PnP + RANSAC | `cv2.solvePnPRansac` |\n",
    "| **Bundle adjustment** | Levenberg-Marquardt | Minimise $\\sum\\|\\mathbf{x} - \\pi(\\mathbf{X})\\|^2$ |\n",
    "\n",
    "### Production SfM Systems\n",
    "For real applications consider these battle-tested libraries:\n",
    "- **COLMAP** — https://colmap.github.io  (C++/Python, the state of the art)\n",
    "- **OpenSfM** — https://github.com/mapillary/OpenSfM  (Python, used by Mapillary)\n",
    "- **Meshroom** — https://alicevision.org  (GUI-based, open source)\n",
    "- **OpenMVG** — https://github.com/openMVG/openMVG  (research-oriented)\n",
    "\n",
    "### Further Reading\n",
    "- Hartley & Zisserman — *Multiple View Geometry in Computer Vision* (the textbook)\n",
    "- Snavely et al. — *Photo Tourism* (SIGGRAPH 2006) — origin of modern SfM\n",
    "- Schönberger & Frahm — *Structure-from-Motion Revisited* (CVPR 2016) — COLMAP paper"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "gpuType": "T4",
   "name": "Structure_from_Motion_Complete.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
